# ğŸš€ AlingAi Pro 6.0 - éƒ¨ç½²å’Œè¿ç»´æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›AlingAi Pro 6.0é›¶ä¿¡ä»»é‡å­åŠ å¯†ç³»ç»Ÿçš„å®Œæ•´éƒ¨ç½²å’Œè¿ç»´æŒ‡å¯¼ï¼Œæ¶µç›–å¼€å‘ã€æµ‹è¯•ã€é¢„ç”Ÿäº§å’Œç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²é…ç½®ã€æ€§èƒ½ä¼˜åŒ–ã€ç›‘æ§å‘Šè­¦å’Œæ•…éšœå¤„ç†ã€‚

---

## ğŸ¯ ç¯å¢ƒè§„åˆ’

### ç¯å¢ƒæ¶æ„

```
å¼€å‘ç¯å¢ƒ (Development)
â”œâ”€â”€ æœ¬åœ°å¼€å‘ç¯å¢ƒ
â”œâ”€â”€ åŠŸèƒ½æµ‹è¯•ç¯å¢ƒ
â””â”€â”€ å•å…ƒæµ‹è¯•ç¯å¢ƒ

æµ‹è¯•ç¯å¢ƒ (Testing)
â”œâ”€â”€ é›†æˆæµ‹è¯•ç¯å¢ƒ
â”œâ”€â”€ æ€§èƒ½æµ‹è¯•ç¯å¢ƒ
â””â”€â”€ å®‰å…¨æµ‹è¯•ç¯å¢ƒ

é¢„ç”Ÿäº§ç¯å¢ƒ (Staging)
â”œâ”€â”€ ç”Ÿäº§ä»¿çœŸç¯å¢ƒ
â”œâ”€â”€ ç”¨æˆ·éªŒæ”¶æµ‹è¯•ç¯å¢ƒ
â””â”€â”€ å‹åŠ›æµ‹è¯•ç¯å¢ƒ

ç”Ÿäº§ç¯å¢ƒ (Production)
â”œâ”€â”€ ä¸»ç”Ÿäº§ç¯å¢ƒ
â”œâ”€â”€ ç¾å¤‡ç¯å¢ƒ
â””â”€â”€ è¾¹ç¼˜èŠ‚ç‚¹ç¯å¢ƒ
```

### æœåŠ¡å™¨è§„æ ¼å»ºè®®

#### å¼€å‘/æµ‹è¯•ç¯å¢ƒ
```yaml
CPU: 4æ ¸å¿ƒ
å†…å­˜: 8GB
å­˜å‚¨: 100GB SSD
ç½‘ç»œ: 100Mbps
æ“ä½œç³»ç»Ÿ: Ubuntu 22.04 LTS / CentOS 8
```

#### ç”Ÿäº§ç¯å¢ƒ - å°å‹éƒ¨ç½²
```yaml
åº”ç”¨æœåŠ¡å™¨:
  CPU: 8æ ¸å¿ƒ (Intel Xeon æˆ– AMD EPYC)
  å†…å­˜: 16GB
  å­˜å‚¨: 200GB NVMe SSD
  ç½‘ç»œ: 1Gbps

æ•°æ®åº“æœåŠ¡å™¨:
  CPU: 16æ ¸å¿ƒ
  å†…å­˜: 32GB
  å­˜å‚¨: 500GB NVMe SSD (RAID 10)
  ç½‘ç»œ: 1Gbps

ç¼“å­˜æœåŠ¡å™¨:
  CPU: 4æ ¸å¿ƒ
  å†…å­˜: 16GB
  å­˜å‚¨: 100GB SSD
  ç½‘ç»œ: 1Gbps
```

#### ç”Ÿäº§ç¯å¢ƒ - å¤§å‹éƒ¨ç½²
```yaml
è´Ÿè½½å‡è¡¡å™¨: (2å°ï¼Œé«˜å¯ç”¨)
  CPU: 8æ ¸å¿ƒ
  å†…å­˜: 16GB
  ç½‘ç»œ: 10Gbps

åº”ç”¨æœåŠ¡å™¨é›†ç¾¤: (4å°ï¼Œæ¨ªå‘æ‰©å±•)
  CPU: 16æ ¸å¿ƒ
  å†…å­˜: 32GB
  å­˜å‚¨: 500GB NVMe SSD
  ç½‘ç»œ: 10Gbps

æ•°æ®åº“é›†ç¾¤: (ä¸»ä»å¤åˆ¶ + è¯»å†™åˆ†ç¦»)
  ä¸»åº“: 32æ ¸å¿ƒï¼Œ64GBå†…å­˜ï¼Œ1TB NVMe SSD
  ä»åº“: 16æ ¸å¿ƒï¼Œ32GBå†…å­˜ï¼Œ500GB NVMe SSD

ç¼“å­˜é›†ç¾¤: (Redis Cluster)
  3å°ç¼“å­˜èŠ‚ç‚¹ï¼Œæ¯å°8æ ¸å¿ƒï¼Œ32GBå†…å­˜
```

---

## ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²

### Dockerç¯å¢ƒé…ç½®

#### 1. åŸºç¡€é•œåƒæ„å»º
```dockerfile
# Dockerfile.prod
FROM php:8.2-fpm-alpine

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apk add --no-cache \
    git \
    zip \
    unzip \
    curl \
    libpng-dev \
    libjpeg-turbo-dev \
    freetype-dev \
    oniguruma-dev \
    libxml2-dev \
    openssl-dev \
    libzip-dev \
    postgresql-dev \
    redis \
    supervisor

# å®‰è£…PHPæ‰©å±•
RUN docker-php-ext-configure gd --with-freetype --with-jpeg \
    && docker-php-ext-install -j$(nproc) \
        gd \
        pdo \
        pdo_mysql \
        pdo_pgsql \
        mbstring \
        xml \
        zip \
        bcmath \
        opcache \
        pcntl \
        sockets

# å®‰è£…Redisæ‰©å±•
RUN pecl install redis && docker-php-ext-enable redis

# å®‰è£…GMPæ‰©å±•ï¼ˆSM2ç®—æ³•éœ€è¦ï¼‰
RUN apk add gmp-dev && docker-php-ext-install gmp

# å®‰è£…Composer
COPY --from=composer:latest /usr/bin/composer /usr/bin/composer

# é…ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . /app

# å®‰è£…ä¾èµ–
RUN composer install --no-dev --optimize-autoloader

# è®¾ç½®æƒé™
RUN chown -R www-data:www-data /app/storage /app/bootstrap/cache

# å¤åˆ¶é…ç½®æ–‡ä»¶
COPY docker/php/php.ini /usr/local/etc/php/conf.d/custom.ini
COPY docker/php/opcache.ini /usr/local/etc/php/conf.d/opcache.ini
COPY docker/supervisord.conf /etc/supervisor/conf.d/supervisord.conf

# æš´éœ²ç«¯å£
EXPOSE 9000

# å¯åŠ¨å‘½ä»¤
CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/conf.d/supervisord.conf"]
```

#### 2. ç”Ÿäº§ç¯å¢ƒ Docker Compose
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # Nginxåå‘ä»£ç†
  nginx:
    image: nginx:alpine
    container_name: alingai-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx/prod.conf:/etc/nginx/nginx.conf:ro
      - ./public:/app/public:ro
      - ./ssl:/etc/ssl/certs:ro
    depends_on:
      - app
    networks:
      - alingai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # PHPåº”ç”¨æœåŠ¡å™¨
  app:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: alingai-app
    environment:
      - APP_ENV=production
      - DB_HOST=mysql
      - REDIS_HOST=redis
      - PHP_OPCACHE_ENABLE=1
      - PHP_MEMORY_LIMIT=512M
    volumes:
      - ./storage:/app/storage
      - ./bootstrap/cache:/app/bootstrap/cache
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - alingai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "php", "/app/artisan", "health:check"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MySQLæ•°æ®åº“
  mysql:
    image: mysql:8.0
    container_name: alingai-mysql
    environment:
      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD}
      MYSQL_DATABASE: ${DB_DATABASE}
      MYSQL_USER: ${DB_USERNAME}
      MYSQL_PASSWORD: ${DB_PASSWORD}
    volumes:
      - mysql-data:/var/lib/mysql
      - ./docker/mysql/custom.cnf:/etc/mysql/conf.d/custom.cnf:ro
      - ./docker/mysql/init:/docker-entrypoint-initdb.d:ro
    networks:
      - alingai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${DB_ROOT_PASSWORD}"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: alingai-redis
    command: redis-server /etc/redis/redis.conf
    volumes:
      - redis-data:/data
      - ./docker/redis/prod.conf:/etc/redis/redis.conf:ro
    networks:
      - alingai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ä»»åŠ¡é˜Ÿåˆ—å¤„ç†å™¨
  queue:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: alingai-queue
    command: php /app/artisan queue:work --sleep=3 --tries=3
    environment:
      - APP_ENV=production
      - DB_HOST=mysql
      - REDIS_HOST=redis
    volumes:
      - ./storage:/app/storage
    depends_on:
      - mysql
      - redis
    networks:
      - alingai-network
    restart: unless-stopped

  # è®¡åˆ’ä»»åŠ¡è°ƒåº¦å™¨
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: alingai-scheduler
    command: php /app/artisan schedule:work
    environment:
      - APP_ENV=production
      - DB_HOST=mysql
      - REDIS_HOST=redis
    volumes:
      - ./storage:/app/storage
    depends_on:
      - mysql
      - redis
    networks:
      - alingai-network
    restart: unless-stopped

networks:
  alingai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  mysql-data:
    driver: local
  redis-data:
    driver: local
```

### Kuberneteséƒ¨ç½²

#### 1. å‘½åç©ºé—´é…ç½®
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: alingai-pro
  labels:
    name: alingai-pro
    environment: production
```

#### 2. åº”ç”¨éƒ¨ç½²é…ç½®
```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alingai-app
  namespace: alingai-pro
  labels:
    app: alingai-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: alingai-app
  template:
    metadata:
      labels:
        app: alingai-app
    spec:
      containers:
      - name: alingai-app
        image: alingai/alingai-pro:6.0.0
        ports:
        - containerPort: 9000
        env:
        - name: APP_ENV
          value: "production"
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: database-secret
              key: host
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: database-secret
              key: password
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 9000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 9000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: storage
          mountPath: /app/storage
        - name: config
          mountPath: /app/config/production
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: alingai-storage
      - name: config
        configMap:
          name: alingai-config
```

#### 3. æœåŠ¡å’ŒIngressé…ç½®
```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: alingai-app-service
  namespace: alingai-pro
spec:
  selector:
    app: alingai-app
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9000
  type: ClusterIP

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: alingai-ingress
  namespace: alingai-pro
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - api.alingai.com
    secretName: alingai-tls
  rules:
  - host: api.alingai.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: alingai-app-service
            port:
              number: 80
```

---

## âš™ï¸ ç³»ç»Ÿé…ç½®ä¼˜åŒ–

### Nginxé…ç½®ä¼˜åŒ–

```nginx
# docker/nginx/prod.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # æ—¥å¿—æ ¼å¼
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    access_log /var/log/nginx/access.log main;

    # æ€§èƒ½ä¼˜åŒ–
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    server_tokens off;
    
    # Gzipå‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml;

    # ç¼“å†²åŒºè®¾ç½®
    client_body_buffer_size 128k;
    client_max_body_size 100m;
    client_header_buffer_size 1k;
    large_client_header_buffers 4 4k;
    output_buffers 1 32k;
    postpone_output 1460;

    # è¿æ¥æ± 
    upstream alingai_backend {
        least_conn;
        server app:9000 weight=1 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=auth:10m rate=5r/s;

    server {
        listen 80;
        listen [::]:80;
        server_name api.alingai.com;
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        listen [::]:443 ssl http2;
        server_name api.alingai.com;
        root /app/public;
        index index.php;

        # SSLé…ç½®
        ssl_certificate /etc/ssl/certs/alingai.crt;
        ssl_certificate_key /etc/ssl/certs/alingai.key;
        ssl_session_timeout 1d;
        ssl_session_cache shared:SSL:50m;
        ssl_session_tickets off;

        # ç°ä»£SSLé…ç½®
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256;
        ssl_prefer_server_ciphers off;

        # HSTS
        add_header Strict-Transport-Security "max-age=63072000" always;

        # å®‰å…¨å¤´
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        add_header Referrer-Policy "strict-origin-when-cross-origin";

        # é™æ€æ–‡ä»¶ç¼“å­˜
        location ~* \.(jpg|jpeg|png|gif|ico|css|js|pdf|txt)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # APIé™æµ
        location /api/auth {
            limit_req zone=auth burst=10 nodelay;
            try_files $uri $uri/ /index.php?$query_string;
        }

        location /api/ {
            limit_req zone=api burst=20 nodelay;
            try_files $uri $uri/ /index.php?$query_string;
        }

        # PHPå¤„ç†
        location ~ \.php$ {
            try_files $uri =404;
            fastcgi_split_path_info ^(.+\.php)(/.+)$;
            fastcgi_pass alingai_backend;
            fastcgi_index index.php;
            include fastcgi_params;
            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
            fastcgi_param PATH_INFO $fastcgi_path_info;
            
            # FastCGIä¼˜åŒ–
            fastcgi_buffer_size 128k;
            fastcgi_buffers 4 256k;
            fastcgi_busy_buffers_size 256k;
            fastcgi_connect_timeout 60s;
            fastcgi_send_timeout 60s;
            fastcgi_read_timeout 60s;
        }

        # éšè—æ•æ„Ÿæ–‡ä»¶
        location ~ /\. {
            deny all;
        }

        location ~ /(composer|package)\.json$ {
            deny all;
        }
    }
}
```

### MySQLé…ç½®ä¼˜åŒ–

```ini
# docker/mysql/custom.cnf
[mysqld]
# åŸºç¡€è®¾ç½®
port = 3306
bind-address = 0.0.0.0
default-storage-engine = InnoDB
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci

# è¿æ¥è®¾ç½®
max_connections = 1000
max_connect_errors = 1000000
back_log = 500
thread_cache_size = 100
interactive_timeout = 300
wait_timeout = 300

# ç¼“å†²åŒºè®¾ç½®
innodb_buffer_pool_size = 8G
innodb_buffer_pool_instances = 8
innodb_log_file_size = 1G
innodb_log_files_in_group = 2
innodb_log_buffer_size = 256M

# æ€§èƒ½ä¼˜åŒ–
innodb_flush_log_at_trx_commit = 2
innodb_flush_method = O_DIRECT
innodb_io_capacity = 1000
innodb_io_capacity_max = 2000
innodb_read_io_threads = 8
innodb_write_io_threads = 8

# æŸ¥è¯¢ç¼“å­˜
query_cache_type = OFF
query_cache_size = 0

# æ…¢æŸ¥è¯¢æ—¥å¿—
slow_query_log = ON
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 2
log_queries_not_using_indexes = ON

# äºŒè¿›åˆ¶æ—¥å¿—
log-bin = mysql-bin
binlog_format = ROW
expire_logs_days = 7
max_binlog_size = 100M

# å®‰å…¨è®¾ç½®
skip-name-resolve
sql_mode = STRICT_TRANS_TABLES,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO

[mysql]
default-character-set = utf8mb4

[client]
default-character-set = utf8mb4
```

### Redisé…ç½®ä¼˜åŒ–

```conf
# docker/redis/prod.conf
# ç½‘ç»œé…ç½®
bind 0.0.0.0
port 6379
tcp-backlog 511
timeout 300
tcp-keepalive 300

# å†…å­˜é…ç½®
maxmemory 4gb
maxmemory-policy allkeys-lru
maxmemory-samples 5

# æŒä¹…åŒ–é…ç½®
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /data

# AOFé…ç½®
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# æ—¥å¿—é…ç½®
loglevel notice
logfile /var/log/redis/redis.log

# å®¢æˆ·ç«¯é…ç½®
maxclients 10000

# å®‰å…¨é…ç½®
requirepass your_redis_password
rename-command FLUSHDB ""
rename-command FLUSHALL ""
rename-command DEBUG ""
rename-command CONFIG "CONFIG_b835f8d7d8f7a8"

# æ€§èƒ½ä¼˜åŒ–
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
hll-sparse-max-bytes 3000
stream-node-max-bytes 4096
stream-node-max-entries 100
```

### PHPé…ç½®ä¼˜åŒ–

```ini
# docker/php/php.ini
[PHP]
engine = On
short_open_tag = Off
precision = 14
output_buffering = 4096
zlib.output_compression = Off
implicit_flush = Off
unserialize_callback_func =
serialize_precision = -1
disable_functions = exec,passthru,shell_exec,system,proc_open,popen
disable_classes =
zend.enable_gc = On
zend.exception_ignore_args = On

# èµ„æºé™åˆ¶
max_execution_time = 300
max_input_time = 300
memory_limit = 512M
post_max_size = 100M
upload_max_filesize = 100M
max_file_uploads = 20

# é”™è¯¯å¤„ç†
display_errors = Off
display_startup_errors = Off
log_errors = On
log_errors_max_len = 1024
ignore_repeated_errors = Off
ignore_repeated_source = Off
report_memleaks = On
error_log = /var/log/php/php_errors.log

# æ•°æ®å¤„ç†
variables_order = "GPCS"
request_order = "GP"
register_argc_argv = Off
auto_globals_jit = On
default_mimetype = "text/html"
default_charset = "UTF-8"

# è·¯å¾„å’Œç›®å½•
include_path = ".:/usr/local/lib/php"
doc_root =
user_dir =
enable_dl = Off

# æ–‡ä»¶ä¸Šä¼ 
file_uploads = On
upload_tmp_dir = /tmp
max_file_uploads = 20

# ä¼šè¯
session.save_handler = redis
session.save_path = "tcp://redis:6379?auth=your_redis_password"
session.use_strict_mode = 1
session.use_cookies = 1
session.use_only_cookies = 1
session.name = ALINGAI_SESSID
session.auto_start = 0
session.cookie_lifetime = 0
session.cookie_path = /
session.cookie_domain =
session.cookie_httponly = 1
session.cookie_secure = 1
session.cookie_samesite = Strict
session.serialize_handler = php_serialize
session.gc_probability = 1
session.gc_divisor = 1000
session.gc_maxlifetime = 1440

# å®‰å…¨
allow_url_fopen = Off
allow_url_include = Off
expose_php = Off
```

```ini
# docker/php/opcache.ini
[opcache]
; å¯ç”¨OPcache
opcache.enable = 1
opcache.enable_cli = 1

; å†…å­˜è®¾ç½®
opcache.memory_consumption = 256
opcache.interned_strings_buffer = 16
opcache.max_accelerated_files = 10000

; æ€§èƒ½è®¾ç½®
opcache.revalidate_freq = 0
opcache.validate_timestamps = 0
opcache.fast_shutdown = 1
opcache.save_comments = 0

; æ–‡ä»¶ç¼“å­˜
opcache.file_cache = /tmp/opcache
opcache.file_cache_only = 0

; JITè®¾ç½® (PHP 8.0+)
opcache.jit_buffer_size = 100M
opcache.jit = tracing

; æ—¥å¿—è®¾ç½®
opcache.log_verbosity_level = 2
opcache.error_log = /var/log/php/opcache.log
```

---

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### ç³»ç»Ÿç›‘æ§é…ç½®

#### 1. Prometheusé…ç½®
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # AlingAiåº”ç”¨ç›‘æ§
  - job_name: 'alingai-app'
    static_configs:
      - targets: ['app:9000']
    metrics_path: '/metrics'
    scrape_interval: 30s

  # Nginxç›‘æ§
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:9113']

  # MySQLç›‘æ§
  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql-exporter:9104']

  # Redisç›‘æ§
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # èŠ‚ç‚¹ç›‘æ§
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

#### 2. å‘Šè­¦è§„åˆ™é…ç½®
```yaml
# monitoring/alert_rules.yml
groups:
  - name: alingai_alerts
    rules:
      # åº”ç”¨å‘Šè­¦
      - alert: ApplicationDown
        expr: up{job="alingai-app"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "AlingAiåº”ç”¨æœåŠ¡ä¸‹çº¿"
          description: "åº”ç”¨æœåŠ¡å·²ä¸‹çº¿è¶…è¿‡1åˆ†é’Ÿ"

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "åº”ç”¨é”™è¯¯ç‡è¿‡é«˜"
          description: "5åˆ†é’Ÿå†…é”™è¯¯ç‡è¶…è¿‡10%"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "å“åº”æ—¶é—´è¿‡é•¿"
          description: "95%çš„è¯·æ±‚å“åº”æ—¶é—´è¶…è¿‡1ç§’"

      # ç³»ç»Ÿèµ„æºå‘Šè­¦
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "CPUä½¿ç”¨ç‡è¿‡é«˜"
          description: "CPUä½¿ç”¨ç‡è¶…è¿‡80%"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡85%"

      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "ç£ç›˜ç©ºé—´ä¸è¶³"
          description: "ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡90%"

      # æ•°æ®åº“å‘Šè­¦
      - alert: MySQLDown
        expr: up{job="mysql"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "MySQLæ•°æ®åº“ä¸‹çº¿"
          description: "MySQLæ•°æ®åº“å·²ä¸‹çº¿"

      - alert: MySQLSlowQueries
        expr: rate(mysql_global_status_slow_queries[5m]) > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "MySQLæ…¢æŸ¥è¯¢è¿‡å¤š"
          description: "æ…¢æŸ¥è¯¢æ•°é‡è¶…è¿‡é˜ˆå€¼"

      - alert: MySQLConnections
        expr: mysql_global_status_threads_connected / mysql_global_variables_max_connections * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "MySQLè¿æ¥æ•°è¿‡é«˜"
          description: "MySQLè¿æ¥æ•°è¶…è¿‡æœ€å¤§è¿æ¥æ•°çš„80%"

      # Rediså‘Šè­¦
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redisç¼“å­˜ä¸‹çº¿"
          description: "Redisç¼“å­˜æœåŠ¡å·²ä¸‹çº¿"

      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Rediså†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "Rediså†…å­˜ä½¿ç”¨ç‡è¶…è¿‡90%"
```

### æ—¥å¿—ç›‘æ§

#### 1. Filebeaté…ç½®
```yaml
# monitoring/filebeat.yml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /app/storage/logs/*.log
    fields:
      service: alingai-app
    fields_under_root: true
    multiline.pattern: '^\[\d{4}-\d{2}-\d{2}'
    multiline.negate: true
    multiline.match: after

  - type: log
    enabled: true
    paths:
      - /var/log/nginx/access.log
    fields:
      service: nginx-access
    fields_under_root: true

  - type: log
    enabled: true
    paths:
      - /var/log/nginx/error.log
    fields:
      service: nginx-error
    fields_under_root: true

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "alingai-logs-%{+yyyy.MM.dd}"

processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded

logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

#### 2. Logstashé…ç½®
```ruby
# monitoring/logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [service] == "alingai-app" {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] %{WORD:level}\: %{GREEDYDATA:log_message}" }
    }
    
    if [level] == "ERROR" or [level] == "CRITICAL" {
      mutate {
        add_tag => ["error"]
      }
    }
  }
  
  if [service] == "nginx-access" {
    grok {
      match => { "message" => '%{IPORHOST:clientip} - %{USER:ident} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent} rt=%{NUMBER:request_time:float}' }
    }
    
    if [response] >= 400 {
      mutate {
        add_tag => ["http_error"]
      }
    }
    
    if [request_time] > 1.0 {
      mutate {
        add_tag => ["slow_request"]
      }
    }
  }
  
  date {
    match => [ "timestamp", "yyyy-MM-dd HH:mm:ss", "dd/MMM/yyyy:HH:mm:ss Z" ]
    target => "@timestamp"
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "alingai-logs-%{+YYYY.MM.dd}"
  }
  
  if "error" in [tags] or "http_error" in [tags] {
    stdout {
      codec => rubydebug
    }
  }
}
```

---

## ğŸ”„ CI/CDæµæ°´çº¿

### GitLab CI/CDé…ç½®

```yaml
# .gitlab-ci.yml
stages:
  - test
  - security
  - build
  - deploy
  - verify

variables:
  DOCKER_REGISTRY: registry.alingai.com
  DOCKER_IMAGE: alingai/alingai-pro
  KUBERNETES_NAMESPACE: alingai-pro

# ä»£ç è´¨é‡æ£€æŸ¥
code_quality:
  stage: test
  image: php:8.2-cli
  before_script:
    - apt-get update && apt-get install -y git zip unzip
    - curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer
    - composer install --dev
  script:
    - vendor/bin/phpstan analyse --level=8 src/
    - vendor/bin/php-cs-fixer fix --dry-run --diff
    - vendor/bin/phpunit --coverage-text --colors=never
  coverage: '/^\s*Lines:\s*\d+.\d+\%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

# å®‰å…¨æ‰«æ
security_scan:
  stage: security
  image: owasp/zap2docker-stable
  script:
    - zap-baseline.py -t http://localhost:8000 -J zap-report.json
  artifacts:
    reports:
      sast: zap-report.json
  allow_failure: true

# æ„å»ºé•œåƒ
build_image:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build -t $DOCKER_REGISTRY/$DOCKER_IMAGE:$CI_COMMIT_SHA -f Dockerfile.prod .
    - docker push $DOCKER_REGISTRY/$DOCKER_IMAGE:$CI_COMMIT_SHA
    - docker tag $DOCKER_REGISTRY/$DOCKER_IMAGE:$CI_COMMIT_SHA $DOCKER_REGISTRY/$DOCKER_IMAGE:latest
    - docker push $DOCKER_REGISTRY/$DOCKER_IMAGE:latest
  only:
    - main

# éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
deploy_staging:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context staging
    - kubectl set image deployment/alingai-app alingai-app=$DOCKER_REGISTRY/$DOCKER_IMAGE:$CI_COMMIT_SHA -n $KUBERNETES_NAMESPACE
    - kubectl rollout status deployment/alingai-app -n $KUBERNETES_NAMESPACE
  environment:
    name: staging
    url: https://staging.api.alingai.com
  only:
    - main

# è‡ªåŠ¨åŒ–æµ‹è¯•
integration_test:
  stage: verify
  image: node:16
  script:
    - npm install -g newman
    - newman run tests/postman/api-tests.json --environment tests/postman/staging.json
  dependencies:
    - deploy_staging
  only:
    - main

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy_production:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context production
    - kubectl set image deployment/alingai-app alingai-app=$DOCKER_REGISTRY/$DOCKER_IMAGE:$CI_COMMIT_SHA -n $KUBERNETES_NAMESPACE
    - kubectl rollout status deployment/alingai-app -n $KUBERNETES_NAMESPACE
  environment:
    name: production
    url: https://api.alingai.com
  when: manual
  only:
    - main

# ç”Ÿäº§ç¯å¢ƒéªŒè¯
production_verify:
  stage: verify
  image: curlimages/curl:latest
  script:
    - curl -f https://api.alingai.com/health || exit 1
    - curl -f https://api.alingai.com/metrics || exit 1
  dependencies:
    - deploy_production
  only:
    - main
```

### GitHub Actionsé…ç½®

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: alingai/alingai-pro

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: secret
          MYSQL_DATABASE: alingai_test
        options: >-
          --health-cmd="mysqladmin ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3

    steps:
    - uses: actions/checkout@v3

    - name: Setup PHP
      uses: shivammathur/setup-php@v2
      with:
        php-version: 8.2
        extensions: mbstring, xml, ctype, iconv, intl, pdo_mysql, redis, gmp
        tools: composer:v2

    - name: Cache Composer dependencies
      uses: actions/cache@v3
      with:
        path: /tmp/composer-cache
        key: ${{ runner.os }}-${{ hashFiles('**/composer.lock') }}

    - name: Install dependencies
      run: composer install --prefer-dist --no-progress

    - name: Copy environment file
      run: cp .env.testing .env

    - name: Generate application key
      run: php artisan key:generate

    - name: Run database migrations
      run: php artisan migrate --force

    - name: Execute tests
      run: vendor/bin/phpunit --coverage-clover coverage.xml

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  build:
    needs: [test, security]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        file: ./Dockerfile.prod
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Deploy to Kubernetes
      uses: azure/k8s-deploy@v1
      with:
        manifests: |
          k8s/deployment.yaml
          k8s/service.yaml
          k8s/ingress.yaml
        images: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
        kubectl-version: 'latest'
```

---

## ğŸ› ï¸ æ•…éšœå¤„ç†å’Œæ¢å¤

### å¸¸è§æ•…éšœè¯Šæ–­

#### 1. åº”ç”¨å¯åŠ¨å¤±è´¥
```bash
# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker ps -a

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs alingai-app

# æ£€æŸ¥é…ç½®æ–‡ä»¶
docker exec alingai-app php artisan config:show

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
docker exec alingai-app php artisan db:check

# æ£€æŸ¥æ–‡ä»¶æƒé™
docker exec alingai-app ls -la storage/
```

#### 2. æ•°æ®åº“è¿æ¥é—®é¢˜
```bash
# æ£€æŸ¥MySQLçŠ¶æ€
docker exec alingai-mysql mysqladmin ping

# æŸ¥çœ‹æ•°æ®åº“æ—¥å¿—
docker logs alingai-mysql

# æ£€æŸ¥è¿æ¥æ•°
docker exec alingai-mysql mysql -u root -p -e "SHOW PROCESSLIST;"

# æ£€æŸ¥é”ç­‰å¾…
docker exec alingai-mysql mysql -u root -p -e "SHOW ENGINE INNODB STATUS\G" | grep -A 10 "LATEST DETECTED DEADLOCK"
```

#### 3. ç¼“å­˜é—®é¢˜
```bash
# æ£€æŸ¥RedisçŠ¶æ€
docker exec alingai-redis redis-cli ping

# æŸ¥çœ‹Redisä¿¡æ¯
docker exec alingai-redis redis-cli info

# æ¸…ç†ç¼“å­˜
docker exec alingai-app php artisan cache:clear
docker exec alingai-app php artisan config:clear
docker exec alingai-app php artisan route:clear
```

#### 4. æ€§èƒ½é—®é¢˜è¯Šæ–­
```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
top
htop
iotop
vmstat 1

# æŸ¥çœ‹è¿›ç¨‹çŠ¶æ€
ps aux | grep php
ps aux | grep nginx
ps aux | grep mysql

# æ£€æŸ¥ç½‘ç»œè¿æ¥
netstat -tulpn
ss -tulpn

# åˆ†ææ…¢æŸ¥è¯¢
docker exec alingai-mysql mysqldumpslow /var/log/mysql/slow.log
```

### å¤‡ä»½å’Œæ¢å¤

#### 1. æ•°æ®åº“å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# backup.sh

# é…ç½®å˜é‡
DB_HOST="mysql"
DB_USER="backup_user"
DB_PASS="backup_password"
DB_NAME="alingai_pro"
BACKUP_DIR="/backup/mysql"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=7

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# æ‰§è¡Œå¤‡ä»½
docker exec alingai-mysql mysqldump \
  --host=$DB_HOST \
  --user=$DB_USER \
  --password=$DB_PASS \
  --single-transaction \
  --routines \
  --triggers \
  --events \
  --create-options \
  --comments \
  --dump-date \
  $DB_NAME | gzip > $BACKUP_DIR/alingai_${DATE}.sql.gz

# æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
if [ -f "$BACKUP_DIR/alingai_${DATE}.sql.gz" ]; then
    echo "å¤‡ä»½æˆåŠŸ: $BACKUP_DIR/alingai_${DATE}.sql.gz"
    
    # éªŒè¯å¤‡ä»½æ–‡ä»¶
    gunzip -t $BACKUP_DIR/alingai_${DATE}.sql.gz
    if [ $? -eq 0 ]; then
        echo "å¤‡ä»½æ–‡ä»¶éªŒè¯æˆåŠŸ"
    else
        echo "å¤‡ä»½æ–‡ä»¶éªŒè¯å¤±è´¥"
        exit 1
    fi
else
    echo "å¤‡ä»½å¤±è´¥"
    exit 1
fi

# æ¸…ç†æ—§å¤‡ä»½
find $BACKUP_DIR -name "alingai_*.sql.gz" -mtime +$RETENTION_DAYS -delete
echo "æ¸…ç†äº†è¶…è¿‡ $RETENTION_DAYS å¤©çš„æ—§å¤‡ä»½æ–‡ä»¶"

# ä¸Šä¼ åˆ°è¿œç¨‹å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
# aws s3 cp $BACKUP_DIR/alingai_${DATE}.sql.gz s3://your-backup-bucket/mysql/
```

#### 2. æ•°æ®æ¢å¤è„šæœ¬
```bash
#!/bin/bash
# restore.sh

# æ£€æŸ¥å‚æ•°
if [ $# -ne 1 ]; then
    echo "ä½¿ç”¨æ–¹æ³•: $0 <å¤‡ä»½æ–‡ä»¶è·¯å¾„>"
    exit 1
fi

BACKUP_FILE=$1
DB_HOST="mysql"
DB_USER="root"
DB_PASS="root_password"
DB_NAME="alingai_pro"

# æ£€æŸ¥å¤‡ä»½æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -f "$BACKUP_FILE" ]; then
    echo "é”™è¯¯: å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $BACKUP_FILE"
    exit 1
fi

# åœæ­¢åº”ç”¨æœåŠ¡
echo "åœæ­¢åº”ç”¨æœåŠ¡..."
docker stop alingai-app alingai-queue alingai-scheduler

# åˆ›å»ºæ¢å¤å‰å¤‡ä»½
echo "åˆ›å»ºæ¢å¤å‰å¤‡ä»½..."
./backup.sh

# æ‰§è¡Œæ¢å¤
echo "å¼€å§‹æ¢å¤æ•°æ®åº“..."
if [[ $BACKUP_FILE == *.gz ]]; then
    gunzip -c $BACKUP_FILE | docker exec -i alingai-mysql mysql \
        --host=$DB_HOST \
        --user=$DB_USER \
        --password=$DB_PASS \
        $DB_NAME
else
    docker exec -i alingai-mysql mysql \
        --host=$DB_HOST \
        --user=$DB_USER \
        --password=$DB_PASS \
        $DB_NAME < $BACKUP_FILE
fi

if [ $? -eq 0 ]; then
    echo "æ•°æ®åº“æ¢å¤æˆåŠŸ"
else
    echo "æ•°æ®åº“æ¢å¤å¤±è´¥"
    exit 1
fi

# å¯åŠ¨åº”ç”¨æœåŠ¡
echo "å¯åŠ¨åº”ç”¨æœåŠ¡..."
docker start alingai-app alingai-queue alingai-scheduler

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 30

# éªŒè¯æ¢å¤
echo "éªŒè¯æ¢å¤ç»“æœ..."
curl -f http://localhost/health
if [ $? -eq 0 ]; then
    echo "æ¢å¤éªŒè¯æˆåŠŸï¼Œåº”ç”¨æ­£å¸¸è¿è¡Œ"
else
    echo "æ¢å¤éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥åº”ç”¨çŠ¶æ€"
fi
```

### ç¾éš¾æ¢å¤è®¡åˆ’

#### 1. æ¢å¤æ—¶é—´ç›®æ ‡ (RTO)
- **å…³é”®ä¸šåŠ¡**: 1å°æ—¶å†…æ¢å¤
- **é‡è¦ä¸šåŠ¡**: 4å°æ—¶å†…æ¢å¤
- **ä¸€èˆ¬ä¸šåŠ¡**: 24å°æ—¶å†…æ¢å¤

#### 2. æ¢å¤ç‚¹ç›®æ ‡ (RPO)
- **æ•°æ®åº“**: 15åˆ†é’Ÿå†…çš„æ•°æ®æŸå¤±
- **æ–‡ä»¶å­˜å‚¨**: 1å°æ—¶å†…çš„æ•°æ®æŸå¤±
- **é…ç½®æ–‡ä»¶**: ç«‹å³æ¢å¤ï¼ˆç‰ˆæœ¬æ§åˆ¶ï¼‰

#### 3. æ¢å¤æ­¥éª¤
```bash
# 1. è¯„ä¼°æŸå®³ç¨‹åº¦
./scripts/assess_damage.sh

# 2. å¯åŠ¨å¤‡ç”¨ç¯å¢ƒ
kubectl apply -f k8s/disaster-recovery/

# 3. æ¢å¤æ•°æ®åº“
./scripts/restore_database.sh latest

# 4. æ¢å¤åº”ç”¨æ–‡ä»¶
./scripts/restore_files.sh

# 5. éªŒè¯ç³»ç»ŸåŠŸèƒ½
./scripts/verify_system.sh

# 6. åˆ‡æ¢DNSæŒ‡å‘
./scripts/switch_dns.sh

# 7. é€šçŸ¥ç›¸å…³äººå‘˜
./scripts/notify_recovery.sh
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å’Œæ‰©å±•

### æ°´å¹³æ‰©å±•ç­–ç•¥

#### 1. åº”ç”¨æœåŠ¡å™¨æ‰©å±•
```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: alingai-app-hpa
  namespace: alingai-pro
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: alingai-app
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

#### 2. æ•°æ®åº“è¯»å†™åˆ†ç¦»
```php
// config/database.php
'mysql' => [
    'write' => [
        'host' => env('DB_WRITE_HOST', 'mysql-master'),
        'port' => env('DB_WRITE_PORT', 3306),
        'database' => env('DB_DATABASE'),
        'username' => env('DB_USERNAME'),
        'password' => env('DB_PASSWORD'),
    ],
    'read' => [
        [
            'host' => env('DB_READ_HOST_1', 'mysql-slave-1'),
            'port' => env('DB_READ_PORT_1', 3306),
            'database' => env('DB_DATABASE'),
            'username' => env('DB_READ_USERNAME'),
            'password' => env('DB_READ_PASSWORD'),
        ],
        [
            'host' => env('DB_READ_HOST_2', 'mysql-slave-2'),
            'port' => env('DB_READ_PORT_2', 3306),
            'database' => env('DB_DATABASE'),
            'username' => env('DB_READ_USERNAME'),
            'password' => env('DB_READ_PASSWORD'),
        ],
    ],
    'sticky' => true,
]
```

#### 3. Redisé›†ç¾¤é…ç½®
```conf
# redis-cluster.conf
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
cluster-announce-ip 10.0.0.1
cluster-announce-port 7000
cluster-announce-bus-port 17000
```

### ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

#### 1. å¤šçº§ç¼“å­˜æ¶æ„
```php
class MultiLevelCache
{
    private $l1Cache; // æœ¬åœ°ç¼“å­˜ (APCu)
    private $l2Cache; // Redisç¼“å­˜
    private $l3Cache; // åˆ†å¸ƒå¼ç¼“å­˜ (Memcached)
    
    public function get(string $key, callable $callback = null, int $ttl = 3600)
    {
        // L1ç¼“å­˜æŸ¥æ‰¾
        $value = $this->l1Cache->get($key);
        if ($value !== false) {
            return $value;
        }
        
        // L2ç¼“å­˜æŸ¥æ‰¾
        $value = $this->l2Cache->get($key);
        if ($value !== false) {
            $this->l1Cache->set($key, $value, min($ttl, 300)); // L1ç¼“å­˜5åˆ†é’Ÿ
            return $value;
        }
        
        // L3ç¼“å­˜æŸ¥æ‰¾
        $value = $this->l3Cache->get($key);
        if ($value !== false) {
            $this->l2Cache->setex($key, $ttl, $value);
            $this->l1Cache->set($key, $value, min($ttl, 300));
            return $value;
        }
        
        // æ‰§è¡Œå›è°ƒè·å–æ•°æ®
        if ($callback) {
            $value = $callback();
            if ($value !== null) {
                $this->set($key, $value, $ttl);
            }
            return $value;
        }
        
        return null;
    }
    
    public function set(string $key, $value, int $ttl = 3600): void
    {
        $this->l3Cache->set($key, $value, $ttl);
        $this->l2Cache->setex($key, $ttl, $value);
        $this->l1Cache->set($key, $value, min($ttl, 300));
    }
}
```

#### 2. æ™ºèƒ½ç¼“å­˜é¢„çƒ­
```php
class CacheWarmer
{
    public function warmup(): void
    {
        $this->warmupUserSessions();
        $this->warmupFrequentQueries();
        $this->warmupStaticData();
    }
    
    private function warmupUserSessions(): void
    {
        // é¢„çƒ­æ´»è·ƒç”¨æˆ·ä¼šè¯
        $activeUsers = User::where('last_activity', '>', now()->subHours(24))->get();
        foreach ($activeUsers as $user) {
            Cache::remember("user_permissions_{$user->id}", 3600, function() use ($user) {
                return $user->getAllPermissions();
            });
        }
    }
    
    private function warmupFrequentQueries(): void
    {
        // é¢„çƒ­é¢‘ç¹æŸ¥è¯¢çš„æ•°æ®
        Cache::remember('system_config', 86400, function() {
            return SystemConfig::all()->pluck('value', 'key');
        });
        
        Cache::remember('api_routes', 3600, function() {
            return Route::getRoutes();
        });
    }
}
```

### æ•°æ®åº“ä¼˜åŒ–

#### 1. æŸ¥è¯¢ä¼˜åŒ–
```sql
-- åˆ›å»ºå¿…è¦çš„ç´¢å¼•
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_status_created ON users(status, created_at);
CREATE INDEX idx_api_tokens_user_expires ON api_tokens(user_id, expires_at);
CREATE INDEX idx_logs_level_created ON logs(level, created_at);

-- å¤åˆç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_user_logs_compound ON user_logs(user_id, action, created_at DESC);

-- å…¨æ–‡ç´¢å¼•
CREATE FULLTEXT INDEX idx_documents_content ON documents(title, content);
```

#### 2. åˆ†åŒºè¡¨è®¾è®¡
```sql
-- æŒ‰æ—¶é—´åˆ†åŒºçš„æ—¥å¿—è¡¨
CREATE TABLE user_logs (
    id BIGINT AUTO_INCREMENT,
    user_id INT NOT NULL,
    action VARCHAR(100) NOT NULL,
    ip_address VARCHAR(45),
    user_agent TEXT,
    created_at TIMESTAMP NOT NULL,
    PRIMARY KEY (id, created_at),
    INDEX idx_user_action (user_id, action)
) PARTITION BY RANGE (UNIX_TIMESTAMP(created_at)) (
    PARTITION p202501 VALUES LESS THAN (UNIX_TIMESTAMP('2025-02-01')),
    PARTITION p202502 VALUES LESS THAN (UNIX_TIMESTAMP('2025-03-01')),
    PARTITION p202503 VALUES LESS THAN (UNIX_TIMESTAMP('2025-04-01')),
    PARTITION p202504 VALUES LESS THAN (UNIX_TIMESTAMP('2025-05-01')),
    PARTITION p202505 VALUES LESS THAN (UNIX_TIMESTAMP('2025-06-01')),
    PARTITION p202506 VALUES LESS THAN (UNIX_TIMESTAMP('2025-07-01')),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

---

## ğŸ“ è¿ç»´æ”¯æŒ

### 24/7è¿ç»´æ£€æŸ¥æ¸…å•

#### æ—¥å¸¸æ£€æŸ¥é¡¹ç›®
```bash
#!/bin/bash
# daily_check.sh

echo "=== AlingAi Pro 6.0 æ—¥å¸¸æ£€æŸ¥æŠ¥å‘Š ===" 
echo "æ£€æŸ¥æ—¶é—´: $(date)"
echo

# 1. ç³»ç»ŸçŠ¶æ€æ£€æŸ¥
echo "1. ç³»ç»ŸçŠ¶æ€æ£€æŸ¥"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
echo

# 2. ç£ç›˜ç©ºé—´æ£€æŸ¥
echo "2. ç£ç›˜ç©ºé—´æ£€æŸ¥"
df -h | grep -v tmpfs
echo

# 3. å†…å­˜ä½¿ç”¨æ£€æŸ¥
echo "3. å†…å­˜ä½¿ç”¨æ£€æŸ¥"
free -h
echo

# 4. æ•°æ®åº“çŠ¶æ€æ£€æŸ¥
echo "4. æ•°æ®åº“çŠ¶æ€æ£€æŸ¥"
docker exec alingai-mysql mysqladmin status
echo

# 5. RedisçŠ¶æ€æ£€æŸ¥
echo "5. RedisçŠ¶æ€æ£€æŸ¥"
docker exec alingai-redis redis-cli info server | grep uptime
echo

# 6. åº”ç”¨å¥åº·æ£€æŸ¥
echo "6. åº”ç”¨å¥åº·æ£€æŸ¥"
curl -s -o /dev/null -w "HTTPçŠ¶æ€ç : %{http_code}, å“åº”æ—¶é—´: %{time_total}s\n" http://localhost/health
echo

# 7. é”™è¯¯æ—¥å¿—æ£€æŸ¥
echo "7. æœ€è¿‘1å°æ—¶é”™è¯¯æ—¥å¿—"
docker logs alingai-app --since 1h 2>&1 | grep -i error | wc -l
echo

# 8. å®‰å…¨äº‹ä»¶æ£€æŸ¥
echo "8. å®‰å…¨äº‹ä»¶æ£€æŸ¥"
tail -n 10 storage/logs/security.log
echo

echo "=== æ£€æŸ¥å®Œæˆ ==="
```

### å‘Šè­¦è”ç³»äººé…ç½®

```yaml
# monitoring/alertmanager.yml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@alingai.com'

route:
  group_by: ['alertname']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      severity: warning
    receiver: 'warning-alerts'

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://webhook.alingai.com/alerts'

- name: 'critical-alerts'
  email_configs:
  - to: 'ops-team@alingai.com'
    subject: 'ğŸš¨ AlingAi Pro å…³é”®å‘Šè­¦'
    body: |
      å‘Šè­¦è¯¦æƒ…:
      {{ range .Alerts }}
      - {{ .Annotations.summary }}
        æè¿°: {{ .Annotations.description }}
      {{ end }}
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#alerts-critical'
    title: 'AlingAi Pro å…³é”®å‘Šè­¦'

- name: 'warning-alerts'
  email_configs:
  - to: 'dev-team@alingai.com'
    subject: 'âš ï¸ AlingAi Pro è­¦å‘Š'
```

---

**Â© 2025 AlingAi Pro 6.0 - ä¼ä¸šçº§éƒ¨ç½²å’Œè¿ç»´æŒ‡å—**
